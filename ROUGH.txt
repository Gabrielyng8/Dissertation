2.1 Computer Vision as a Measurement and Monitoring Tool

The field of computer vision exists to develop systems that allow computers to extract useful data from visual content, including images and video footage. Computer vision operates as an inverse problem which requires scientists to extract physical world properties from visual data that contains noise and missing information [1]. The operation of vision-based systems depends on sensor noise, lighting changes, occlusions and restricted sensor resolution which create fundamental uncertainty in their performance.

Computer vision serves engineering and inspection operations through its ability to perform measurements and monitoring tasks instead of its capacity for semantic interpretation. Camera-based systems provide automated non-contact physical process observation which produces repeatable objective results for quality assurance and calibration operations that require automated inspection, as manual observation becomes inconsistent and subjective [1]. These systems operate at high speed to handle extensive data collection which enables researchers to perform quantitative data evaluation.

Computer vision functions across a range from basic image processing operations to advanced image analysis and interpretation methods. The field of image processing deals with operations which accept images as input and produce images as output but computer vision specializes in retrieving information and structural relationships and attributes from visual content [3]. The stages in applied systems operate together to establish stable visual data which enables accurate measurement processes.

When utilizing video as a data source, temporal sampling becomes a critical priority. For moving objects, measurement accuracy is heavily dependent on frame rate, exposure settings, and the management of motion blur. Robust vision systems must therefore account for both spatial and temporal effects to generate the precise, time stamped observations required for subsequent analysis [1], [2].

2.2 Object Detection and Visual Tracking

For video analysis, it is necessary to make computers capable of extracting and accurately locating objects and other visual details in still images and, if necessary, tracking them across consecutive frames. This ability is often discussed within the computer vision literature using the related concepts of object detection and tracking. Vision-based measurement systems are inherently affected by calibration accuracy, sensor noise, and perspective distortion. These factors have a considerable impact on the precision, complexity of the algorithms and robustness of the measurement system.

In vision-based systems that support statistical analysis, inaccuracies introduced during visual localisation propagate into subsequent geometric interpretation and statistical inference, directly affecting the reliability of calibration and monitoring outcomes. A crucial step in the design of such systems is the choice between using a detection-only strategy and one which tracks visual features or objects over time.

Video analysis poses problems not present in still image analysis: motion blur, varying illumination, partial occlusion and fast object movement. This can often be achieved by using temporal models which take advantage of continuity between the individual frames. The degree to which explicit tracking is required depends on the properties of the observed system in question. In constrained mechanical systems such as mechanical amusement park devices, the shapes are rigid and the movement is in circles. In constrained visual scenes these features have a significant impact on the trade-offs encountered.

Object detection in images is fundamentally different from tracking of objects in video streams. The distinction lies in the way the data is available to the algorithms. In the context of mechanically constrained, cyclic systems it is crucial to understand how the paradigms in question affect system design. In the following sections, we examine the application of various detection approaches, from conventional image processing techniques to the more recent learning based methods, considering these requirements.

    2.2.1 Object Detection versus Visual Tracking

    Both object detection and tracking are crucial components of video analysis. However, they deal with different aspects of video content. Object detection focuses on determining the location of objects within a static image. On the other hand, tracking follows the motion of identified objects over time. The task of object detection is essentially to localise objects in specific parts of a given picture, the location being one of the key pieces of information which is needed for object detection. Processing for each frame is typically done in isolation which generates outputs such as the masks, bounding boxes or keypoints without making use of any temporal context in those particular processes [1], [4].

    Traditional optical tracking relies on the presence of a feature in two successive images, and is limited by the quality of the images, and the availability of a feature in both images. Optical tracking also suffers from the presence of noise, in that a feature may be present in one image but not in another, due to variations in illumination or other factors. The task is to track the position of an object through time, taking into account the position of the object in each frame of the sequence. This may involve relating the observations made in each image and using motion models to forecast where the object will be on the next frame [4]. Among traditional tracking techniques, Kalman filters, as well as the particle filters and optical flow techniques, are frequently found. Conversely, contemporary tracking systems typically involve tracking by detection, combining per-frame detectors with temporal association mechanisms [5].

    In complex environments where there is no constraint and objects are subject to changes in appearance or occlusion through interaction with other objects, detection and tracking is vital. Currently, tracking-by-detection frameworks, for example SORT and its variations, employ probabilistic motion models and data association algorithms to correlate detections between frames. This results in greater temporal stability and fewer missed object detections in scenes which are highly populated or in dynamic conditions [5]. However, these benefits come at the cost of increased algorithmic complexity and the introduction of new error modes, such as identity switches and cumulative drift.

    From the engineering viewpoint, the requirement for explicit state estimation is dependent on the characteristics of the system under observation. In rotational systems where the motion is constrained mechanically, the position of moving parts can be fixed and their movements can be anticipated because the pattern of motion is predictable. In such systems, detections are defined relative to fixed spatial regions of interest corresponding to known geometric features, rather than through continuous object trajectory estimation. In many instances, frame-by-frame re-identification, coupled with precise time stamping and correct ordering of the output, can provide the temporal resolution required without the overhead of continuous tracking.

    Methods employing detection alone are simple and easy to understand. Since frame by frame detection takes place, error does not gradually accumulate over time, and thus failure can be directly attributed to certain conditions such as movement blur or light changes rather than to the system's internal state. In calibration and monitoring processes, it is especially important that the output of the system is transparent. This is because the goal is to detect mechanical drift or a geometric deviation rather than tracking objects.

    In fact, pipelines which merely detect events require the detector to be more robust and require more frequent sampling. Temporal smoothing techniques can be used to reduce this form of noise from occurring by introducing a form of delay between the measurement and the use of that measurement in the control process. While these restrictions can be considerable, they can be alleviated through higher frame rates and confidence thresholding. Alternatively, further analysis may be performed using geometric constraints, allowing tracking to be carried out less frequently without compromising the results.

    Video analysis can be achieved through two different techniques, either by tracking visual objects or by detecting objects in images. In interactive environments with lots of things happening at the same time, being able to track something is very important. However in cases where parts of a machine move back and forth in a repeating cycle, it is often sufficient and advantageous to have the object detected. The choice of methods that are visual in nature is guided by the fact that outcomes are rotational. This choice is made clear in the next sections where image processing techniques are used in the detection of objects. These techniques are based on traditional methods and also those which are learning based.

    2.2.2 Classical Image Processing Techniques

    In controlled environments where lighting, object appearance, and scene structure stay relatively unchanged, classical image processing techniques offer a set of deterministic, non-learning-based methods for image feature extraction. In contrast to learning-based approaches, these methods are based on analytical models. These do not require learning from a dataset and are less computationally intensive and more comprehensible.

    Template matching methods make use of a predefined image patch, called a template, to search through a larger image. It uses a similarity measure to compare the template with patches within the larger image. Due to their partial robustness to variations in global intensity and contrast, correlation-based similarity measures are often used, specifically normalised cross-correlation (NCC) [6]. The computational cost of normalized cross-correlation is reduced through efficient formulations, and this is achieved without a reduction in detection accuracy, thus allowing it to be used in real time [6].

    It is worth noting that template matching is sensitive to variations in the size of objects, as well as to variations in the objects' orientation and viewpoint. This happens because small template variations can affect the alignment of the template and the target [7]. This limitation has led researchers to investigate a multi-template strategy where separate templates for the varying orientations of the object are separately matched. Registration algorithms can be extended to account for rotation and scale variations, this is achieved by applying an FFT-based registration method before cross-correlation [8]. However, these approaches increase the computational load and remain constrained by the discretisation of variations in pose.

    Alongside spatial matching, another technique which is commonly used to simplify detection in such controlled environments is colour-based segmentation. The HSV model has the benefit of being able to separate the brightness of the light from the chromatic content, yielding more stable object detection results than thresholding based on RGB [9]. Efficient isolation of objects with distinctive colour can be achieved using a standard OpenCV pipeline involving morphological filtering and colour thresholds for hue, saturation and value [9]. This process is computationally inexpensive and easily understood, making it an appropriate choice for real-time vision systems and those which are embedded.

    Traditional processing of images has the benefits of simplicity of design, transparency and the lack of any necessity for training examples. Low-complexity estimators have several characteristics that make them useful for applications with restricted computational power or with predictable system conditions. Their main limitation is their susceptibility to changes in the environment, including variations in lighting, objects blocking the camera's view and movements in the scene which were not taken into consideration [7]. The effectiveness of these methods is heavily dependent on acquiring the data under strict controls and proper parameter adjustment.

    2.2.3 Learning-Based Detection Methods (YOLO Family)

    Modern object detection is commonly divided into two-stage and single-stage pipelines. Generally single-stage detectors, like SSD and YOLO, classify objects directly without preliminary region-proposal steps. However, these direct approaches typically sacrifice some level of accuracy for the sake of speed, especially in more complex scenarios. In single-stage detectors, class probabilities and bounding box coordinates are output directly in a single forward pass. This speed-accuracy trade-off is evidently shown through comparative benchmarking with visual corruption. Clean images are typically where single-stage models like YOLO will excel, but severe corruption can cause two-stage approaches to be more stable, which is a trade-off that can be influenced by the choice of training strategy and data augmentation [10].

    Single-stage detectors such as SSD have shown that object detection can be achieved in real-time without the use of pre-processed object proposals. This is done by predicting class probabilities and bounding box offsets directly over multiple scale feature maps. This multi-scale prediction technique is especially useful when objects are at varying sizes in a scene, because the model's performance is improved when it has access to data from various resolutions. The unified training and evaluation process in the SSD reduces the need for region proposal networks, thereby simplifying the system and allowing high-speed processing of video data [11].

    Within single-stage detection, the YOLO family is notable for prioritising the balancing of localisation precision with processing speed, which results in sufficient object localisation accuracy in real-world systems. Current developments in the YOLO project, particularly in versions YOLOv8 and above, represent a shift towards the use of network architectures that increase small object detection capabilities and improve the stability of optimisation without compromising real-time processing speeds. A YOLOv8-style model can be considered to be composed of three main components: input pre-processing, a backbone, and prediction components. The prediction components comprise a neck for multi-scale feature fusion and a head for final object prediction. The pre-processing stage typically incorporates sophisticated data augmentation techniques, such as mosaicing or tile-and-blend style image mixing, to improve the network’s generalisation capabilities. Pre-processing can also take place at this point to improve the neural network's generalisation capabilities. In models such as YOLOv8, the backbone extracts hierarchical features, typically through the use of CSP-style designs that incorporate a C2f module. The neck of the model fuses multi-scale features according to FPN/PAN principles. Ultimately, the network's head is responsible for producing output related to classification and the bounding box's location. YOLOv8 uses a decoupled head which separates the tasks of classification and regression in order to reduce dependence on the hand tuning of parameters and to make it easier to adapt to new data sets [12].

    One of the major advantages of YOLOv8-style detectors is that they are capable of real-time processing. This is a key reason why they are of use in the monitoring of high frame rates. Studies of rotating machinery inspections indicate that the YOLOv8 model can be modified to make a portable system by combining motion stabilisation techniques with two other optimisation methods: the use of faster neural network algorithms and the use of faster computer chips. Using temporal compensation, an edge-based system detects rotating machinery, eliminating inter frame motion effects with optical flow. It uses a customised version of YOLO that incorporates depthwise separable convolution and feature fusion and this setup provides a real-time capability on the platform. This type of design is particularly relevant in the capture of roulette-style images where fast motion, camera vibration and the effect of blur can otherwise lower the detection accuracy and lead to missed targets [14].

    A significant challenge in automatically recording roulette outcomes lies in the small scale of the target objects, specifically the roulette ball or winning marker. This can be a problem because it can occupy a very small area on the image, especially if the camera is not positioned closely to the game or the entire roulette layout is not visible. Small objects are particularly affected by downsampling and the loss of feature detail in object detection tasks using YOLOv8. This means that such tasks need better combining of data from different feature scales and appropriate data augmentations [12], [13]. Such approaches concentrate on small objects by employing certain architectural adjustments intended to maintain small object features. For example, HP-YOLOv8 introduces modifications to (a) feature extraction modules, (b) attention-based feature fusion, and (c) bounding-box regression loss design to better represent small targets under clutter and overlap. While this research does not aim to replicate the specific modules of HP-YOLOv8, it adopts the underlying principle to justify why roulette-ball detection benefits from (i) stronger multi-scale features, (ii) attention/feature fusion strategies, and (iii) loss/assignment choices that do not collapse when the object is tiny or partially occluded [13]. 

    Roulette images often contain motion blur, reflections, changing lighting conditions and sometimes partial occlusion. As a result, data augmentation should be considered at the design stage for this kind of object detection system. Experiments in robustness benchmarking have shown that models trained and tested under abnromalities (e.g., blur, fog-like effects, Gaussian or salt-and-pepper noise) exhibit clear differences in how the performance degrades, and that the "best" model can change depending on whether the operating environment is clean or visually corrupted. These results highlight a trade-off that is of importance: robustness typically requires additional computational requirements (possibly more compute, more augmentation, careful benchmarking). This is in contrast to lightweight variants which can sometimes perform better under corruption despite achieving lower peak accuracy on clean images [10]. In the context of a roulette vision pipeline this allows the image augmentation to be considered as a controlled variable in the design, aligned with failure modes like blur, glare and low light.

    These algorithms should be tested using performance criteria appropriate for a high-speed rotational measurement context as well as standard measures of accuracy. In this roulette detection problem, we wish to ensure that the system is able to continuously and correctly identify the ball’s location across successive frames, maintain accurate position throughout a sequence of images and that inference latency is consistent, ensuring that predictions remain synchronized with the physical system. Studies of rotational machinery show that pre-processing, including compensation for motion, may be necessary when dealing with camera shake or the target moving quickly. Research on detection of small objects indicates that it is advantageous to use a network architecture that can handle objects at various scales. While the analysis lends weight to selecting a YOLOv8+ style detector as a strong baseline for the identification of roulette outcomes, it also suggests that improving robustness (through the use of data augmentations and stochastic regularisation as well as model tuning) is at the cost of increased complexity and experimentation time [10], [14].


SOURCES:

[1] R. Szeliski, “Computer Vision: Algorithms and Applications, 2nd ed. (excerpt pages 1–10),” final draft Sept. 2021. [Online]. Available: https://www.cs.utexas.edu/~pstone/Courses/309fall24/preclass/readings/week6_Szeliski_CVAA_Book_p_1_10.pdf. Accessed: Dec. 13, 2025.
[2] G. Bradski and A. Kaehler, Learning OpenCV: Computer Vision with the OpenCV Library. Sebastopol, CA, USA: O’Reilly Media, 2008. [Online]. Available: https://www.eecs.yorku.ca/course_archive/2010-11/W/4421/doc/LearningOpenCV_1_2.pdf. Accessed: Dec. 13, 2025.
[3] R. C. Gonzalez and R. E. Woods, “Chapter 1: Introduction (excerpt),” in Digital Image Processing Using MATLAB, Pearson/Prentice Hall, 2004. [Online]. Available: https://imageprocessingplace.com/downloads_V3/dipum1e_downloads/dipum1e_sample_book_material_downloads/chapter_01_dipum.pdf. Accessed: Dec. 13, 2025.
[4] Z. Fan, Y. Zhu, Y. He, Q. Sun, H. Liu, and J. He, ‘Deep Learning on Monocular Object Pose Detection and Tracking: A Comprehensive Overview’, Apr. 21, 2022, arXiv: arXiv:2105.14291. doi: 10.48550/arXiv.2105.14291.
[5] V. Mandal and Y. Adu-Gyamfi, ‘Object Detection and Tracking Algorithms for Vehicle Counting: A Comparative Analysis’, J. Big Data Anal. Transp., vol. 2, no. 3, pp. 251–261, Dec. 2020, doi: 10.1007/s42421-020-00025-w.
[6] K. Briechle and U. D. Hanebeck, ‘Template matching using fast normalized cross correlation’, presented at the Aerospace/Defense Sensing, Simulation, and Controls, D. P. Casasent and T.-H. Chao, Eds, Orlando, FL, Mar. 2001, pp. 95–102. doi: 10.1117/12.421129.
[7] Y. Han, ‘Reliable Template Matching for Image Detection in Vision Sensor Systems’, Sensors (Basel), vol. 21, no. 24, p. 8176, Dec. 2021, doi: 10.3390/s21248176.
[8] B. S. Reddy and B. N. Chatterji, ‘An FFT-based technique for translation, rotation, and scale-invariant image registration’, IEEE Transactions on Image Processing, vol. 5, no. 8, pp. 1266–1271, Aug. 1996, doi: 10.1109/83.506761.
[9] P. Song, ‘Object Detection based on HSV in OpenCV’, Applied and Computational Engineering, vol. 121, pp. 116–122, Jan. 2025, doi: 10.54254/2755-2721/2025.19740.
[10] S. Gholinavaz, N. Saeedi, and S. S. Gharehveran, ‘Robustness analysis of YOLO and faster R-CNN for object detection in realistic weather scenarios with noise augmentation’, Sci Rep, vol. 15, no. 1, p. 44888, Dec. 2025, doi: 10.1038/s41598-025-28737-5.
[11] W. Liu et al., ‘SSD: Single Shot MultiBox Detector’, in Computer Vision – ECCV 2016, B. Leibe, J. Matas, N. Sebe, and M. Welling, Eds, Cham: Springer International Publishing, 2016, pp. 21–37. doi: 10.1007/978-3-319-46448-0_2.
[12] V. Afifah and S. Erniwati, ‘YOLOv8 for Object Detection: A Comprehensive Review of Advances,Techniques, and Applications’, IJACI : International Journal of Advanced Computing and Informatics, vol. 2, no. 1, pp. 53–61, 2026, doi: 10.71129/ijaci.v2i1.pp53-61.
[13] G. Yao, S. Zhu, L. Zhang, and M. Qi, ‘HP-YOLOv8: High-Precision Small Object Detection Algorithm for Remote Sensing Images’, Sensors, vol. 24, no. 15, p. 4858, Jan. 2024, doi: 10.3390/s24154858.
[14] J. Chen, J. Tong, and J. Su, ‘Design of a real-time abnormal detection system for rotating machinery based on YOLOv8’, Front. Mech. Eng., vol. 11, Oct. 2025, doi: 10.3389/fmech.2025.1683572.