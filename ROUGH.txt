2.1 Computer Vision as a Measurement and Monitoring Tool

The field of computer vision exists to develop systems that allow computers to extract useful data from visual content, including images and video footage. Computer vision operates as an inverse problem which requires scientists to extract physical world properties from visual data that contains noise and missing information [1]. The operation of vision-based systems depends on sensor noise, lighting changes, occlusions and restricted sensor resolution which create fundamental uncertainty in their performance.

Computer vision serves engineering and inspection operations through its ability to perform measurements and monitoring tasks instead of its capacity for semantic interpretation. Camera-based systems provide automated non-contact physical process observation which produces repeatable objective results for quality assurance and calibration operations that require automated inspection, as manual observation becomes inconsistent and subjective [1]. These systems operate at high speed to handle extensive data collection which enables researchers to perform quantitative data evaluation.

Computer vision functions across a range from basic image processing operations to advanced image analysis and interpretation methods. The field of image processing deals with operations which accept images as input and produce images as output but computer vision specializes in retrieving information and structural relationships and attributes from visual content [3]. The stages in applied systems operate together to establish stable visual data which enables accurate measurement processes.

When utilizing video as a data source, temporal sampling becomes a critical priority. For moving objects, measurement accuracy is heavily dependent on frame rate, exposure settings, and the management of motion blur. Robust vision systems must therefore account for both spatial and temporal effects to generate the precise, time stamped observations required for subsequent analysis [1], [2].

2.2 Object Detection and Visual Tracking

For video analysis, it is necessary to make computers capable of extracting and accurately locating objects and other visual details in still images and, if necessary, tracking them across consecutive frames. This ability is often discussed within the computer vision literature using the related concepts of object detection and tracking. Vision-based measurement systems are inherently affected by calibration accuracy, sensor noise, and perspective distortion. These factors have a considerable impact on the precision, complexity of the algorithms and robustness of the measurement system.

In vision-based systems that support statistical analysis, inaccuracies introduced during visual localisation propagate into subsequent geometric interpretation and statistical inference, directly affecting the reliability of calibration and monitoring outcomes. A crucial step in the design of such systems is the choice between using a detection-only strategy and one which tracks visual features or objects over time.

Video analysis poses problems not present in still image analysis: motion blur, varying illumination, partial occlusion and fast object movement. This can often be achieved by using temporal models which take advantage of continuity between the individual frames. The degree to which explicit tracking is required depends on the properties of the observed system in question. In constrained mechanical systems such as mechanical amusement park devices, the shapes are rigid and the movement is in circles. In constrained visual scenes these features have a significant impact on the trade-offs encountered.

Object detection in images is fundamentally different from tracking of objects in video streams. The distinction lies in the way the data is available to the algorithms. In the context of mechanically constrained, cyclic systems it is crucial to understand how the paradigms in question affect system design. In the following sections, we examine the application of various detection approaches, from conventional image processing techniques to the more recent learning based methods, considering these requirements.

2.2.1 Object Detection versus Visual Tracking

    Both object detection and tracking are crucial components of video analysis. However, they deal with different aspects of video content. Object detection focuses on determining the location of objects within a static image. On the other hand, tracking follows the motion of identified objects over time. The task of object detection is essentially to localise objects in specific parts of a given picture, the location being one of the key pieces of information which is needed for object detection. Processing for each frame is typically done in isolation which generates outputs such as the masks, bounding boxes or keypoints without making use of any temporal context in those particular processes [1], [4].

    Traditional optical tracking relies on the presence of a feature in two successive images, and is limited by the quality of the images, and the availability of a feature in both images. Optical tracking also suffers from the presence of noise, in that a feature may be present in one image but not in another, due to variations in illumination or other factors. The task is to track the position of an object through time, taking into account the position of the object in each frame of the sequence. This may involve relating the observations made in each image and using motion models to forecast where the object will be on the next frame [4]. Among traditional tracking techniques, Kalman filters, as well as the particle filters and optical flow techniques, are frequently found. Conversely, contemporary tracking systems typically involve tracking by detection, combining per-frame detectors with temporal association mechanisms [5].

    In complex environments where there is no constraint and objects are subject to changes in appearance or occlusion through interaction with other objects, detection and tracking is vital. Currently, tracking-by-detection frameworks, for example SORT and its variations, employ probabilistic motion models and data association algorithms to correlate detections between frames. This results in greater temporal stability and fewer missed object detections in scenes which are highly populated or in dynamic conditions [5]. However, these benefits come at the cost of increased algorithmic complexity and the introduction of new error modes, such as identity switches and cumulative drift.

    From the engineering viewpoint, the requirement for explicit state estimation is dependent on the characteristics of the system under observation. In rotational systems where the motion is constrained mechanically, the position of moving parts can be fixed and their movements can be anticipated because the pattern of motion is predictable. In such systems, detections are defined relative to fixed spatial regions of interest corresponding to known geometric features, rather than through continuous object trajectory estimation. In many instances, frame-by-frame re-identification, coupled with precise time stamping and correct ordering of the output, can provide the temporal resolution required without the overhead of continuous tracking.

    Methods employing detection alone are simple and easy to understand. Since frame by frame detection takes place, error does not gradually accumulate over time, and thus failure can be directly attributed to certain conditions such as movement blur or light changes rather than to the system's internal state. In calibration and monitoring processes, it is especially important that the output of the system is transparent. This is because the goal is to detect mechanical drift or a geometric deviation rather than tracking objects.

    In fact, pipelines which merely detect events require the detector to be more robust and require more frequent sampling. Temporal smoothing techniques can be used to reduce this form of noise from occurring by introducing a form of delay between the measurement and the use of that measurement in the control process. While these restrictions can be considerable, they can be alleviated through higher frame rates and confidence thresholding. Alternatively, further analysis may be performed using geometric constraints, allowing tracking to be carried out less frequently without compromising the results.

    Video analysis can be achieved through two different techniques, either by tracking visual objects or by detecting objects in images. In interactive environments with lots of things happening at the same time, being able to track something is very important. However in cases where parts of a machine move back and forth in a repeating cycle, it is often sufficient and advantageous to have the object detected. The choice of methods that are visual in nature is guided by the fact that outcomes are rotational. This choice is made clear in the next sections where image processing techniques are used in the detection of objects. These techniques are based on traditional methods and also those which are learning based.

    2.2.2 Classical Image Processing Techniques

    In controlled environments where lighting, object appearance, and scene structure stay relatively unchanged, classical image processing techniques offer a set of deterministic, non-learning-based methods for image feature extraction. In contrast to learning-based approaches, these methods are based on analytical models. These do not require learning from a dataset and are less computationally intensive and more comprehensible.

    Template matching methods make use of a predefined image patch, called a template, to search through a larger image. It uses a similarity measure to compare the template with patches within the larger image. Due to their partial robustness to variations in global intensity and contrast, correlation-based similarity measures are often used, specifically normalised cross-correlation (NCC) [6]. The computational cost of normalized cross-correlation is reduced through efficient formulations, and this is achieved without a reduction in detection accuracy, thus allowing it to be used in real time [6].

    It is worth noting that template matching is sensitive to variations in the size of objects, as well as to variations in the objects' orientation and viewpoint. This happens because small template variations can affect the alignment of the template and the target [7]. This limitation has led researchers to investigate a multi-template strategy where separate templates for the varying orientations of the object are separately matched. Registration algorithms can be extended to account for rotation and scale variations, this is achieved by applying an FFT-based registration method before cross-correlation [8]. However, these approaches increase the computational load and remain constrained by the discretisation of variations in pose.

    Alongside spatial matching, another technique which is commonly used to simplify detection in such controlled environments is colour-based segmentation. The HSV model has the benefit of being able to separate the brightness of the light from the chromatic content, yielding more stable object detection results than thresholding based on RGB [9]. Efficient isolation of objects with distinctive colour can be achieved using a standard OpenCV pipeline involving morphological filtering and colour thresholds for hue, saturation and value [9]. This process is computationally inexpensive and easily understood, making it an appropriate choice for real-time vision systems and those which are embedded.

    Traditional processing of images has the benefits of simplicity of design, transparency and the lack of any necessity for training examples. Low-complexity estimators have several characteristics that make them useful for applications with restricted computational power or with predictable system conditions. Their main limitation is their susceptibility to changes in the environment, including variations in lighting, objects blocking the camera's view and movements in the scene which were not taken into consideration [7]. The effectiveness of these methods is heavily dependent on acquiring the data under strict controls and proper parameter adjustment.

SOURCES:

[1] R. Szeliski, “Computer Vision: Algorithms and Applications, 2nd ed. (excerpt pages 1–10),” final draft Sept. 2021. [Online]. Available: https://www.cs.utexas.edu/~pstone/Courses/309fall24/preclass/readings/week6_Szeliski_CVAA_Book_p_1_10.pdf. Accessed: Dec. 13, 2025.

[2] G. Bradski and A. Kaehler, Learning OpenCV: Computer Vision with the OpenCV Library. Sebastopol, CA, USA: O’Reilly Media, 2008. [Online]. Available: https://www.eecs.yorku.ca/course_archive/2010-11/W/4421/doc/LearningOpenCV_1_2.pdf. Accessed: Dec. 13, 2025.

[3] R. C. Gonzalez and R. E. Woods, “Chapter 1: Introduction (excerpt),” in Digital Image Processing Using MATLAB, Pearson/Prentice Hall, 2004. [Online]. Available: https://imageprocessingplace.com/downloads_V3/dipum1e_downloads/dipum1e_sample_book_material_downloads/chapter_01_dipum.pdf. Accessed: Dec. 13, 2025.
[4] Z. Fan, Y. Zhu, Y. He, Q. Sun, H. Liu, and J. He, ‘Deep Learning on Monocular Object Pose Detection and Tracking: A Comprehensive Overview’, Apr. 21, 2022, arXiv: arXiv:2105.14291. doi: 10.48550/arXiv.2105.14291.
[5] V. Mandal and Y. Adu-Gyamfi, ‘Object Detection and Tracking Algorithms for Vehicle Counting: A Comparative Analysis’, J. Big Data Anal. Transp., vol. 2, no. 3, pp. 251–261, Dec. 2020, doi: 10.1007/s42421-020-00025-w.
[6] K. Briechle and U. D. Hanebeck, ‘Template matching using fast normalized cross correlation’, presented at the Aerospace/Defense Sensing, Simulation, and Controls, D. P. Casasent and T.-H. Chao, Eds, Orlando, FL, Mar. 2001, pp. 95–102. doi: 10.1117/12.421129.
[7] Y. Han, ‘Reliable Template Matching for Image Detection in Vision Sensor Systems’, Sensors (Basel), vol. 21, no. 24, p. 8176, Dec. 2021, doi: 10.3390/s21248176.
[8] B. S. Reddy and B. N. Chatterji, ‘An FFT-based technique for translation, rotation, and scale-invariant image registration’, IEEE Transactions on Image Processing, vol. 5, no. 8, pp. 1266–1271, Aug. 1996, doi: 10.1109/83.506761.
[9] P. Song, ‘Object Detection based on HSV in OpenCV’, Applied and Computational Engineering, vol. 121, pp. 116–122, Jan. 2025, doi: 10.54254/2755-2721/2025.19740.