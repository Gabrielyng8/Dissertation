## 1. **Summary of Meeting Focus**

This meeting centred on reviewing my current progress with **template matching**, discussing limitations of the approach, and exploring a **new direction using YOLO** informed by the context already available in the frame. We also clarified **expectations for the dissertation deliverable** — specifically, that this is _research_, not a polished commercial product.

---
## 2. **Work Presented to Mentor**

### **2.1 Template Matching Progress**

I demonstrated:

- **Basic template-matching example**  
    _Single_ target image + _single_ template.
- **Advanced template-matching example**  
    Provided a _folder_ of templates → system scans automatically.
- **Multi-Rotated Template Generator Script**  
    Used to create a full 360° rotation set for templates.

### **2.2 Current Implementation (Using Red ‘3’ Pocket)**

- Explained how the pipeline works
- Showed detection results
- Highlighted that the **four base template images** contained adjacent pockets, which affects performance.

---
## 3. **Discussion About Limitations & Issues**

### **3.1 Why Adjacent Numbers Appeared in Templates**

I explained:

- When rotating an image, **black corners appear**
- To avoid empty regions, the script must **progressively zoom in** as rotation approaches 45°
- This creates inconsistent crops, often keeping adjacent pockets visible

### **3.2 Mentor’s Concern About Template Matching**

He stated clearly:

> “You will get stuck with template matching if you try it with other video sources.”

Meaning:

- It might work in one controlled video
- But it is _not generalisable_ across wheels, lighting setups, and frame resolutions

---

## 4. **New Insight: YOLO With Contextual Crops**

My mentor noted:

> “Since you have more than the singular ‘3’ pocket, you already have _context_, albeit limited.”

This sparked a new idea:

- Use the same type of images used for template matching (showing a pocket + adjacent pockets)
- Create a **YOLO dataset** of these contextual pocket views
- Annotate _one pocket_ per image (using Roboflow)
- Train a _context-aware_ object detector for pockets

This could drastically simplify detection and improve generalisation.

---

## 5. **Idea Discussed: Using One Pocket to Reconstruct the Entire Wheel**

I presented a geometric idea:

### **If I track _one_ pocket across a full spin:**

- I obtain a **set of coordinates forming an ellipse**
- From that ellipse, I can **mathematically generate 36 pocket positions** around the perimeter

### **Benefits:**

- If tracking is imperfect and yields only a _partial_ ellipse, I can still **complete** the ellipse mathematically
- Could reduce the need for detecting all pockets directly
- Provides a strong geometric calibration step for the pipeline

Mentor found the idea interesting and relevant.

---

## 6. **Clarifying Dissertation Expectations**

I raised a concern:

> “How complete does my final pipeline need to be?  
> Do I need one final polished file that fully automates everything?”

Mentor clarified:

### ✔ This is **research**, not industry production

I am **not** building a deployable commercial tool.

Instead, I am answering:

- _“How far can we push performance with a single, affordable camera?”_
- _“Which computer-vision variables and techniques are viable for detecting wheel outcomes?”_

My focus should be:

- experimentation
- comparison of multiple CV approaches
- prototype results
- reporting accuracy, precision, and limitations

I am not expected to ship a finished product — only to demonstrate:

> “Using this setup and algorithm, I achieved this level of accuracy.”

---

## 7. **Key Outcomes & Next Steps**

- Explore **YOLO dataset creation** using contextual pocket screenshots
- Investigate training a small YOLO model for pocket detection
- Continue refining template-matching insights (for comparison purposes)
- Begin testing the **ellipse-fitting** idea using tracked pocket coordinates
- Keep in mind:
    - This project is about **discovering viability**, not building a perfect solution